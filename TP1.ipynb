{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 1 - Traitement de données\n",
    "\n",
    "Dylan Robins - E2I5 - 17 Dec 2021\n",
    "\n",
    "## Exercice 1: Températures\n",
    "\n",
    "Dans cet exercice, nous analyserons un jeu de données contenant la température moyenne chaque mois dans 35 villes européennes tout au long d'une année."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRARY IMPORT\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from IPython.display import display\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA IMPORT\n",
    "df = pd.read_excel(\"datasets/Temperatures.xlsx\", sheet_name=0, header=0, index_col=0)\n",
    "\n",
    "if df.isnull().values.any():\n",
    "    print(\"Missing data!\")\n",
    "    exit(1)\n",
    "else:\n",
    "    print(\"No missing data!\")\n",
    "\n",
    "print(f\"Dataframe shape: {df.shape[0]} rows, {df.shape[1]} cols\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par faire un plot naif de nos données pour voir si on peut en tirer quelquechose. En l'occurence, à part voir que rien ne se détache trop du reste, on n'a pas grand chose d'interprétable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot()\n",
    "plt.xticks(rotation=45)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant regarder les moyennes et les variances des différentes colonnes du tableau, ainsi que leurs distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot()\n",
    "pd.DataFrame({\"Moyenne\": df.mean(), \"Variance\": df.var()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut maintenant étudier les intercorrélations entre les différentes colonnes. Le scatter_matrix de base de pandas étant illisible quand on a autant de données, on utilisera un heatmap de seaborn. On constate que toutes les variables sont corrélées positivement, bien que les mois aux opposés des solstices le sont plus que les autres (Janv/Dec, Fev/Nov...).\n",
    "\n",
    "On calcule ensuite les coefficients de corrélation et de covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(), vmin=-1, vmax=1, cmap=\"coolwarm\")\n",
    "\n",
    "print(\"Covariance:\")\n",
    "display(df.cov())\n",
    "\n",
    "print(\"\\nCorrélation:\")\n",
    "display(df.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut désormais effectuer une ACP afin de réduire la dimension de ces données. On constate que 98,16% de la variance est expliquée par les deux premiers axes: on pourra donc se réduire à une représentation 2D dans la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acp = PCA()\n",
    "Xproj = acp.fit_transform(df)\n",
    "\n",
    "print(\n",
    "    f\"Variance expliquée par les deux premiers vecteur propres:\",\n",
    "    sum(acp.explained_variance_ratio_[:2])\n",
    ")\n",
    "\n",
    "print(\"Somme des variances:\", sum(acp.explained_variance_))\n",
    "\n",
    "labels = [f\"V{i}\" for i, _ in enumerate(acp.explained_variance_ratio_)]\n",
    "plt.bar(labels, acp.explained_variance_ratio_, width=0.25, label='Variance ratio')\n",
    "plt.plot(labels, acp.explained_variance_ratio_.cumsum(), 'r.-', label='Cumulative sum')\n",
    "plt.title(\"Diagramme de Pareto\")\n",
    "plt.legend()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On trace maintenant le nuage de points de nos données expliquées par les deux premiers vecteurs propres de notre nouvelle base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Xproj[:,0], Xproj[:,1])\n",
    "for i, name in enumerate(df.index):\n",
    "    plt.annotate(name, (Xproj[i,0], Xproj[i,1]), xytext=(2, 2), textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit donc une différenciation claire entre nos villes, cependant nous ne savons pas à quoi correspondent nos axes, si ce n'est que ce sont des combinaisons linéaires entre les différents mois de l'année. Pour obtenir un peu plus de détail sur cette combinaison linéaire, on trace le cercle de corrélation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlations between new data (Xproj) and original columns (df):\n",
    "corvar = np.zeros((len(df.columns), 2))\n",
    "for i, col in enumerate(df.columns):\n",
    "    corvar[i, 0] = np.corrcoef(Xproj[:,0], df.iloc[:, i])[0, 1]\n",
    "    corvar[i, 1] = np.corrcoef(Xproj[:,1], df.iloc[:, i])[0, 1]\n",
    "  \n",
    "cor_df = pd.DataFrame(corvar, columns=[\"1\", \"2\"], index=df.columns)\n",
    "print(f\"Correlation coefficients:\")\n",
    "display(cor_df)\n",
    "\n",
    "# Cercle des corrélations\n",
    "fig, axes = plt.subplots(figsize=(8,8))\n",
    "axes.set_xlim(-1,1)\n",
    "axes.set_ylim(-1,1)\n",
    "\n",
    "# On ajoute les axes\n",
    "plt.plot([-1,1],[0,0],color='silver',linestyle='-',linewidth=1)\n",
    "plt.plot([0,0],[-1,1],color='silver',linestyle='-',linewidth=1)\n",
    "# On ajoute un cercle\n",
    "cercle = plt.Circle((0,0),1,color='blue',fill=False)\n",
    "axes.add_artist(cercle)\n",
    "plt.xlabel(\"Composante principale 1\")\n",
    "plt.ylabel(\"Composante principale 2\")\n",
    "plt.title('Cercle des corrélations')\n",
    "plt.scatter(corvar[:,0],corvar[:,1])\n",
    "#affichage des étiquettes (noms des variables)\n",
    "for j, _ in enumerate(df.columns):\n",
    "  plt.annotate(df.columns[j],(corvar[j,0],corvar[j,1]), xytext=(2, 2), textcoords='offset points')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut donc dire que l'axe 1 correspond majoritairement à l'information \"il fait chaud dans la ville en général\", tandis que l'axe 2 correspond majoritairement à l'information \"les hivers sont doux et les étés froids\".\n",
    "\n",
    "Cela explique bien la position des villes données sur le nuage de points précédent:\n",
    "- Les villes méditerranéennes comme Séville se trouvent à droite,\n",
    "- Rekyavik, ville chauffée par l'activité volcanique de l'Islande mais se situant proche de l'Arctique, se retrouve tout en haut à gauche de notre graphe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2: Criminalité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA IMPORT\n",
    "df = pd.read_excel(\"datasets/Criminalite.xlsx\", sheet_name=0, header=0, index_col=0)\n",
    "\n",
    "if df.isnull().values.any():\n",
    "    print(\"Missing data!\")\n",
    "    exit(1)\n",
    "else:\n",
    "    print(\"No missing data!\")\n",
    "\n",
    "print(f\"Dataframe shape: {df.shape[0]} rows, {df.shape[1]} cols\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encore une fois, nous avons un jeu de données assez conséquent et opaque à première vue. Le simple graphe des données n'est pas très parlant. Appliquons donc le même raisonnement que précédemment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df.plot()\n",
    "plt.xticks(rotation=45)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot()\n",
    "plt.xticks(rotation=45)\n",
    "pd.DataFrame({\"Moyenne\": df.mean(), \"Variance\": df.var()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit dans le boxplot quelques \"outliers\" qui peuvent être intéressantes à étudier. On devrait pouvoir les identifier plus clairement une fois le traitement réalisé.\n",
    "\n",
    "Comme précédemment, on calcule les coefficients de corrélation et on constate que les agressions et les viols sont corrélés positivement tandis que les meurtres et la petite délinquence sont inversement proportionnels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(), vmin=-1, vmax=1, cmap=\"coolwarm\")\n",
    "\n",
    "print(\"\\nCoefficients de corrélation:\")\n",
    "display(df.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACP 1: traitement des données centrées\n",
    "\n",
    "On commence par réaliser un traitement sur les données simplement centrées sur l'origine des axes.\n",
    "\n",
    "Pour centrer une série de données, on lui retire sa moyenne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = df.to_numpy().mean(axis=0)\n",
    "df_centered = df - df.mean()\n",
    "\n",
    "plt.scatter(df.index, df[\"Meurtres\"])\n",
    "plt.scatter(df.index, df_centered[\"Meurtres\"])\n",
    "plt.xticks(rotation=90, fontsize = 8)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nos données sont centrées, on peut procéder à l'ACP. On voit qu'encore une fois, les deux premiers axes nous donnet largement assez d'information pour décrire la variabilité de notre dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acp = PCA()\n",
    "Xproj = acp.fit_transform(df_centered)\n",
    "\n",
    "print(\n",
    "    f\"Variance expliquée par les deux premiers vecteur propres:\",\n",
    "    sum(acp.explained_variance_ratio_[:2])\n",
    ")\n",
    "\n",
    "print(\"Somme des variances:\", sum(acp.explained_variance_))\n",
    "\n",
    "labels = [f\"V{i}\" for i, _ in enumerate(acp.explained_variance_ratio_)]\n",
    "plt.bar(labels, acp.explained_variance_ratio_, width=0.25, label='Variance ratio')\n",
    "plt.plot(labels, acp.explained_variance_ratio_.cumsum(), 'r.-', label='Cumulative sum')\n",
    "plt.title(\"Diagramme de Pareto\")\n",
    "plt.legend()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On trace donc le nuage de points selon ces deux axes ainsi que le cercle de corrélation, et on obtient les graphes suivants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Xproj[:,0], Xproj[:,1])\n",
    "for i, name in enumerate(df_centered.index):\n",
    "    plt.annotate(name, (Xproj[i,0], Xproj[i,1]), xytext=(2, 2), textcoords='offset points', fontsize=8)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Calculate correlations between new data (Xproj) and original columns (df):\n",
    "corvar = np.zeros((len(df_centered.columns), 2))\n",
    "for i, col in enumerate(df_centered.columns):\n",
    "    corvar[i, 0] = np.corrcoef(Xproj[:,0], df_centered.iloc[:, i])[0, 1]\n",
    "    corvar[i, 1] = np.corrcoef(Xproj[:,1], df_centered.iloc[:, i])[0, 1]\n",
    "  \n",
    "# Cercle des corrélations\n",
    "fig, axes = plt.subplots(figsize=(8,8))\n",
    "axes.set_xlim(-1,1)\n",
    "axes.set_ylim(-1,1)\n",
    "\n",
    "# On ajoute les axes\n",
    "plt.plot([-1,1],[0,0],color='silver',linestyle='-',linewidth=1)\n",
    "plt.plot([0,0],[-1,1],color='silver',linestyle='-',linewidth=1)\n",
    "# On ajoute un cercle\n",
    "cercle = plt.Circle((0,0),1,color='blue',fill=False)\n",
    "axes.add_artist(cercle)\n",
    "plt.xlabel(\"Composante principale 1\")\n",
    "plt.ylabel(\"Composante principale 2\")\n",
    "plt.title('Cercle des corrélations')\n",
    "plt.scatter(corvar[:,0],corvar[:,1])\n",
    "#affichage des étiquettes (noms des variables)\n",
    "for j, _ in enumerate(df_centered.columns):\n",
    "  plt.annotate(df_centered.columns[j],(corvar[j,0],corvar[j,1]), xytext=(2, 2), textcoords='offset points')\n",
    "\n",
    "plt.show()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit donc que l'axe principal correspond en quelque sorte à un taux de criminalité général, tandis que le second établit une distinction sur le nombre de vols à la tire.\n",
    "\n",
    "On peut donc dire que\n",
    "- L'Arizona a le plus grand taux de criminalité, le West Virginia le plus faible\n",
    "- Le Wyoming et l'Iowa ont une proportion de vols à la tire supérieure que le Massachussets.\n",
    "\n",
    "On remarque cependant qu'il est difficile de distinguer les Etats entre eux pour ce qui est des crimes autre que le vol à la tire étant donné qu'ils sont tous expliqués de façon presque identique par le 2e axe.\n",
    "\n",
    "### ACP 2: traitement des données centrées / réduites\n",
    "\n",
    "On veut maintenant faire exactement pareil mais avec un jeu de données centré et réduit.\n",
    "\n",
    "Pour centrer/réduire nos données, on leur retire leur moyenne puis on les divise par la racine carrée de leur variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = df.to_numpy().mean(axis=0)\n",
    "df_cr = (df - df.mean()) / df.std()\n",
    "\n",
    "plt.scatter(df.index, df[\"Meurtres\"])\n",
    "plt.scatter(df.index, df_cr[\"Meurtres\"])\n",
    "plt.xticks(rotation=90, fontsize = 8)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acp = PCA()\n",
    "Xproj = acp.fit_transform(df_cr)\n",
    "\n",
    "print(\n",
    "    f\"Variance expliquée par les deux premiers vecteur propres:\",\n",
    "    sum(acp.explained_variance_ratio_[:2])\n",
    ")\n",
    "\n",
    "print(\"Somme des variances:\", sum(acp.explained_variance_))\n",
    "\n",
    "labels = [f\"V{i}\" for i, _ in enumerate(acp.explained_variance_ratio_)]\n",
    "plt.bar(labels, acp.explained_variance_ratio_, width=0.25, label='Variance ratio')\n",
    "plt.plot(labels, acp.explained_variance_ratio_.cumsum(), 'r.-', label='Cumulative sum')\n",
    "plt.title(\"Diagramme de Pareto\")\n",
    "plt.legend()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit ici une différence notable par rapport à la méthode précédente: ici les deux premiers axes ne suffisent pas tout à fait à expliquer l'entièreté de la variabilité, ce seuil étant habituellement définit à 80%. Ici nous avons 76% donc cela reste correct, mais ce serait sûrement plus intéressant de faire des représentations en 3 dimentions pour interpréter nos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Xproj[:,0], Xproj[:,1])\n",
    "for i, name in enumerate(df_cr.index):\n",
    "    plt.annotate(name, (Xproj[i,0], Xproj[i,1]), xytext=(2, 2), textcoords='offset points', fontsize=8)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Calculate correlations between new data (Xproj) and original columns (df):\n",
    "corvar = np.zeros((len(df_cr.columns), 2))\n",
    "for i, col in enumerate(df_centered.columns):\n",
    "    corvar[i, 0] = np.corrcoef(Xproj[:,0], df_centered.iloc[:, i])[0, 1]\n",
    "    corvar[i, 1] = np.corrcoef(Xproj[:,1], df_cr.iloc[:, i])[0, 1]\n",
    "  \n",
    "# Cercle des corrélations\n",
    "fig, axes = plt.subplots(figsize=(8,8))\n",
    "axes.set_xlim(-1,1)\n",
    "axes.set_ylim(-1,1)\n",
    "\n",
    "# On ajoute les axes\n",
    "plt.plot([-1,1],[0,0],color='silver',linestyle='-',linewidth=1)\n",
    "plt.plot([0,0],[-1,1],color='silver',linestyle='-',linewidth=1)\n",
    "# On ajoute un cercle\n",
    "cercle = plt.Circle((0,0),1,color='blue',fill=False)\n",
    "axes.add_artist(cercle)\n",
    "plt.xlabel(\"Composante principale 1\")\n",
    "plt.ylabel(\"Composante principale 2\")\n",
    "plt.title('Cercle des corrélations')\n",
    "plt.scatter(corvar[:,0],corvar[:,1])\n",
    "#affichage des étiquettes (noms des variables)\n",
    "for j, _ in enumerate(df_cr.columns):\n",
    "  plt.annotate(df_cr.columns[j],(corvar[j,0],corvar[j,1]), xytext=(2, 2), textcoords='offset points')\n",
    "\n",
    "plt.show()\n",
    "plt.scatter(Xproj[:,0], Xproj[:,1])\n",
    "for i, name in enumerate(df_cr.index):\n",
    "    plt.annotate(name, (Xproj[i,0], Xproj[i,1]), xytext=(2, 2), textcoords='offset points', fontsize=8)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Calculate correlations between new data (Xproj) and original columns (df):\n",
    "corvar = np.zeros((len(df_cr.columns), 2))\n",
    "for i, col in enumerate(df_centered.columns):\n",
    "    corvar[i, 0] = np.corrcoef(Xproj[:,0], df_centered.iloc[:, i])[0, 1]\n",
    "    corvar[i, 1] = np.corrcoef(Xproj[:,1], df_cr.iloc[:, i])[0, 1]\n",
    "  \n",
    "# Cercle des corrélations\n",
    "fig, axes = plt.subplots(figsize=(8,8))\n",
    "axes.set_xlim(-1,1)\n",
    "axes.set_ylim(-1,1)\n",
    "\n",
    "# On ajoute les axes\n",
    "plt.plot([-1,1],[0,0],color='silver',linestyle='-',linewidth=1)\n",
    "plt.plot([0,0],[-1,1],color='silver',linestyle='-',linewidth=1)\n",
    "# On ajoute un cercle\n",
    "cercle = plt.Circle((0,0),1,color='blue',fill=False)\n",
    "axes.add_artist(cercle)\n",
    "plt.xlabel(\"Composante principale 1\")\n",
    "plt.ylabel(\"Composante principale 2\")\n",
    "plt.title('Cercle des corrélations')\n",
    "plt.scatter(corvar[:,0],corvar[:,1])\n",
    "#affichage des étiquettes (noms des variables)\n",
    "for j, _ in enumerate(df_cr.columns):\n",
    "  plt.annotate(df_cr.columns[j],(corvar[j,0],corvar[j,1]), xytext=(2, 2), textcoords='offset points')\n",
    "\n",
    "plt.show()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "ax.scatter(Xproj[:,0], Xproj[:,1], Xproj[:,2], linewidth=0.2)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "for i, name in enumerate(df_cr.index):\n",
    "    plt.annotate(name, (Xproj[i,0], Xproj[i,1], Xproj[:,2]), xytext=(2, 2, 2), textcoords='offset points', fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que même si seulement 76% de la variabilité est expliquée, ce graphe est beaucoup plus intéressant à interprérer que la version seulement centrée! En effet on a une distinction claire entre les différentes contributions des axes, et on peut voir clairement que l'axe 2 correspond à l'inverse de la gravité des crimes: plus un Etat est bas sur l'axe, plus il y a de meurtres, de viols et d'agressions par opposition à de la petite délinquence.\n",
    "\n",
    "Comme précédemment, on peut donc dire que l'Arizona a un taux de criminalité supérieur à la moyenne, mais qu'en revanche ces crimes sont moins \"graves\" que ceux commis au Mississipi.\n",
    "\n",
    "On voit donc clairement l'intéret de traiter les données centrées/réduites: cela nous permet de mieux distinguer les contributions des différentes variables que l'on souhaite prendre en compte, ce qui facilite grandement l'interprétation des résultats"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d13539d952f813834be5d0878ec8b49565dae7d547b6c46c4ab31b191853e5cc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('anaconda3-2021.05': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
